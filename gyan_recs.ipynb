{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naseehasalam/IRIS_gyanrecs/blob/main/gyan_recs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Python Program to perform summarization on the CNN/Dailymail Dataset**\n",
        "_I have used the csv version of the test data from a Kaggle competition instead of the given CNN/Dailymail dataset URL'S which were provided in the Recruitments document since I couldnt not scrape the article from them properly_\n",
        "\n",
        "In the following python program we perform the tasks given below:-\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZyIFSC55j_gu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data from the CSV file is read using Pandas library"
      ],
      "metadata": {
        "id": "TlQspL3VQTBJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSOKErbsxeqw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df=pd.read_csv(\"test.csv\")\n",
        "\n",
        "article_text=list(df['article'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. The Read data is tokenised using the NLTK library to words and sentences and further filtered."
      ],
      "metadata": {
        "id": "0K_fVcIvQeiS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kirdJZxM1TwA"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize,word_tokenize\n",
        "\n",
        "stop_words=list(stopwords.words('english'))\n",
        "\n",
        "sentences={}\n",
        "for i in range(len(article_text)):\n",
        "  sent=sent_tokenize(article_text[i])\n",
        "  sentences[i]=sent\n",
        "\n",
        "words={}\n",
        "for i in range(len(article_text)):\n",
        "  word=word_tokenize(article_text[i])\n",
        "  words[i]=word\n",
        "\n",
        "for i in words:\n",
        "  for j in words[i]:\n",
        "    if j in stop_words:\n",
        "      words[i].remove(j)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Transformers Library is Installed."
      ],
      "metadata": {
        "id": "m78s-rtCQvdc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abFnWF98_H6z"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.Importing pipelines from the Hugging Face Transformer."
      ],
      "metadata": {
        "id": "9cgyw3CyQ03I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Sr3epXUUADRf"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import pipeline\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5.Chunking the data so that the pipeline is stable"
      ],
      "metadata": {
        "id": "Y__nAxzvRCD4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epD0yOAWBldS"
      },
      "outputs": [],
      "source": [
        "max_chunk=500\n",
        "current_chunk=0\n",
        "chunks=[]\n",
        "\n",
        "for i in sentences:\n",
        "  for j in sentences[i]:\n",
        "    if len(chunks)==current_chunk+1:\n",
        "      if len(chunks[current_chunk]) + len(j.split(' ')) <= max_chunk:\n",
        "        chunks[current_chunk].extend(j.split(' '))\n",
        "      else:\n",
        "        current_chunk+=1\n",
        "        chunks.append(j.split(' '))\n",
        "    else:\n",
        "      print(current_chunk)\n",
        "      chunks.append(j.split(' '))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. The summarization pipeline is created."
      ],
      "metadata": {
        "id": "7NAVyS1RRL-S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tX2LrCyEoGr"
      },
      "outputs": [],
      "source": [
        "summariser=pipeline(\"summarization\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7.Summarising each article."
      ],
      "metadata": {
        "id": "b90I2kT_RdEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summaries=[]\n",
        "for i in chunks:\n",
        "  summ=summariser(i,max_length=150,min_length=30,do_sample=False)\n",
        "  summaries.append(summ)\n",
        "\n",
        "sum2=[]\n",
        "sum_pd=pd.DataFrame(summaries,column=['summary'])\n",
        "sum_pd.to_csv('test.csv',index=False)\n",
        "  "
      ],
      "metadata": {
        "id": "gM4KyWy0Rgw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8.ROGUE score calculation\n"
      ],
      "metadata": {
        "id": "GWH7T6sfT0SM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge\n",
        "from rouge import Rouge\n",
        "rouge = Rouge()\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "df=pd.read_csv(\"test.csv\")\n",
        "\n",
        "article_text=list(df['article'])\n",
        "summaries=list(df['summary'])\n",
        "\n",
        "for i in article_text:\n",
        "  for j in summaries:\n",
        "    scores = rouge.get_scores(j,i)\n",
        "scores"
      ],
      "metadata": {
        "id": "VuN0lwR_SyOr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLH6vlbcGsfDpGS34gbInK",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}